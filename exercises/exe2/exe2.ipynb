{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import count, col, length, asc, udf, lit, floor, pow\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, BooleanType\n",
    "from pyspark.sql import Row\n",
    "import math\n",
    "from itertools import combinations\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data\n"
     ]
    }
   ],
   "source": [
    "print(\"Load the data\")\n",
    "users_queries_search_main_df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .csv(\"user-ct-test-collection-01.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define global variables\n"
     ]
    }
   ],
   "source": [
    "print(\"Define global variables\")\n",
    "n1 = 2 # min num of rows to display\n",
    "n2 = 20 # max num of rows to display\n",
    "confidences = [0.6, 0.8, 0.9, 1] # Sorted list of confidences\n",
    "min_num_of_chars_in_query = 2\n",
    "min_num_of_queries = 6 # Minimum number of occurences of a aingle query.\n",
    "min_num_of_queries_pair =  math.floor(min_num_of_queries * confidences[0]) # Minimum number of occurences of pair of queries for different users.\n",
    "levenshtein_distance_threshhold = 10 # threshold for similarity between 2 queries\n",
    "display_rules_num_of_records_threshhold = 400 # threshold for collecting results as list (in order to display the results on the console in a readable format.)\n",
    "stop_websites = 'google|gmail|mapquest|ebay|myspace|yahoo' # Irrlevant websites, that probably not related to search queries, because of the trend to use them as a home page.\n",
    "stop_queries = ['...', 'null', 'http', 'http;', 'htp', 'thttp', 'ww', 'www', 'www.', 'com', '.com', 'goole', 'goog', 'googl', 'gm..', 'g mail.com', 'g mail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repartition users_queries_df by user column\n",
      "repartition queries_count_df by query column\n",
      "number of users queries count is: 277955\n",
      "+-----+-------+-----------+\n",
      "|query|user   |count_query|\n",
      "+-----+-------+-----------+\n",
      "|ako  |2706422|41         |\n",
      "|ako  |9640439|41         |\n",
      "+-----+-------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_queries_df = users_queries_search_main_df.select('AnonID', 'Query')\\\n",
    "                    .drop_duplicates(subset=['AnonID', 'Query'])\\\n",
    "                    .filter((col('Query').rlike(stop_websites) == False) & (length(col(\"Query\")) >= min_num_of_chars_in_query))\\\n",
    "                    .select(col('AnonID').alias('user'), col('Query').alias('query'))\n",
    "\n",
    "stop_queries_df = spark.createDataFrame(stop_queries, StringType()).toDF(\"query\")\n",
    "unwanted_queries_df = users_queries_df.join(stop_queries_df, on='query' , how = 'inner').select('user', 'query')\n",
    "users_queries_df = users_queries_df.subtract(unwanted_queries_df)\n",
    "\n",
    "print(\"repartition users_queries_df by user column\")\n",
    "users_queries_df.repartition('user')\n",
    "\n",
    "queries_count_df = users_queries_df.groupBy('query').agg(count(\"*\").alias(\"count_query\"))\\\n",
    "                    .filter(\"count_query >= \" + repr(min_num_of_queries))\n",
    "\n",
    "print(\"repartition queries_count_df by query column\")\n",
    "queries_count_df.repartition('query')\n",
    "\n",
    "users_queries_count_df = users_queries_df.join(queries_count_df, on='query', how='inner')\n",
    "num_of_users_queries_count = users_queries_count_df.count()\n",
    "print(\"number of users queries count is: \" + repr(num_of_users_queries_count))\n",
    "users_queries_count_df.show(n1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of pair count queries = 46449\n",
      "[(('love quotes', 'weather'), 3), (('americawest', 'bank of america'), 3), (('msnbc.com', 'dogpile'), 3), (('american idol', 'yellow pages'), 23), (('lowes', 'air tran'), 3), (('mohegan sun', 'bankofamerica'), 3), (('remax', 'american express'), 4), (('newsday', 'qvc'), 3), (('target', 'nelly'), 3), (('msn.com', 'sbcglobal.net'), 4), (('avon', 'sears'), 7), (('southwest.com', 'overstock.com'), 3), (('office max', 'nbc'), 3), (('gay sex', 'sex'), 4), (('dictionary.com', 'dictionary'), 14), (('costco.com', 'home depot'), 5), (('vin diesel', 'american idol'), 3), (('limewire.com', 'sex'), 3), (('map quest', 'six flags'), 3), (('edible arrangements', 'home depot'), 3)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "users_pair_queries_count_rdd is converted to rdd,\n",
    "in order to avoid self cartesian join (which is required in order to get all of the combinations of pair of queries)\n",
    "by performing the following operations:\n",
    ".map: map user to suitable queries. (query, user, count_query) ==> (user, query)\n",
    ".reduceByKey: For each user add the suitable list of queries. ((u1, [q1,q2,...]), (u2, [q2,q5,...]), ...)\n",
    ".map: create pairs of queries combinations, for each user & drop users. ((q1,q2),(q1,q5),(q2,q5)...)\n",
    ".flatMap add 1 to each query pair ((q1,q2),(q1,q5),(q2,q5)...) ==> (((q1,q2),1),((q1,q5),1),((q2,q5),1)...)\n",
    ".reduceByKey count num of occurences of each query pair (((q1,q2),3),((q1,q5),7),((q2,q5),102)...)\n",
    ".filter filter out queries, with number of occurences below min_num_of_queries_pair.\n",
    "        For example: if min_num_of_queries_pair = 6, then the tuple ((q1,q2),3) will be filtered out.\n",
    "'''\n",
    "users_pair_queries_count_rdd = users_queries_count_df.rdd\\\n",
    "                        .map(lambda line: (line[1], [line[0]]))\\\n",
    "                        .reduceByKey(add)\\\n",
    "                        .map(lambda line: tuple(combinations(line[1], 2)))\\\n",
    "                        .flatMap(lambda line: [(x, 1) for x in line])\\\n",
    "                        .reduceByKey(add)\\\n",
    "                        .filter(lambda line: line[1] >= min_num_of_queries_pair)\n",
    "\n",
    "num_of_queries_pairs = users_pair_queries_count_rdd.count()\n",
    "print('num of pair count queries = ' + repr(num_of_queries_pairs))\n",
    "print(users_pair_queries_count_rdd.take(n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of queries count filtered is: 14984\n",
      "+-----+-----------+\n",
      "|query|count_query|\n",
      "+-----+-----------+\n",
      "|ako  |41         |\n",
      "|anime|26         |\n",
      "+-----+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the updated queries count data frame, Don't select user column\n",
    "queries_count_filtered_df = users_queries_count_df.drop_duplicates(subset=['query', 'count_query'])\\\n",
    "                            .select('query', 'count_query')\n",
    "\n",
    "num_of_queries_count_filtered = queries_count_filtered_df.count()\n",
    "print(\"number of queries count filtered is: \" + repr(num_of_queries_count_filtered))\n",
    "queries_count_filtered_df.show(n1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of pair queries results = 46449\n"
     ]
    }
   ],
   "source": [
    "# Convert users_pair_queries_count_rdd to data frame, in order to join it with queries_count_filtered_df\n",
    "users_pair_queries_count_df = sqlContext.createDataFrame(users_pair_queries_count_rdd.map(lambda line: Row(query=line[0][0], query2=line[0][1], count_2_queries=line[1])))\n",
    "num_of_pair_queries_count = users_pair_queries_count_df.count()\n",
    "print('num of pair queries results = ' + repr(num_of_pair_queries_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join query count to results data frame\n",
      "num of results = 155\n",
      "+-----------------+----------+---------------+-----------+\n",
      "|query            |query2    |count_2_queries|count_query|\n",
      "+-----------------+----------+---------------+-----------+\n",
      "|craigslist boston|craigslist|5              |7          |\n",
      "|cia              |fbi       |8              |12         |\n",
      "+-----------------+----------+---------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Join query count to results data frame\")\n",
    "results_df1 = users_pair_queries_count_df.join(queries_count_filtered_df, on = 'query', how = 'inner')\\\n",
    "                                        .filter('count_2_queries / count_query >= ' + repr(confidences[0]))\\\n",
    "                                        .select('query', 'query2', 'count_2_queries', 'count_query')\n",
    "\n",
    "results_df2 = users_pair_queries_count_df.join(queries_count_filtered_df.select(col('query').alias('query2'), col('count_query')), on = 'query2', how = 'inner')\\\n",
    "                                        .filter('count_2_queries / count_query >= ' + repr(confidences[0]))\\\n",
    "                                        .select('query', 'query2', 'count_2_queries', 'count_query')\n",
    "\n",
    "results_df = results_df1.union(results_df2)\n",
    "num_of_results = results_df.count()\n",
    "print('num of results = ' + repr(num_of_results))\n",
    "results_df.show(n1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s, t):\n",
    "        ''' From Wikipedia article; Iterative with two matrix rows. '''\n",
    "        if s == t: return 0\n",
    "        elif len(s) == 0: return len(t)\n",
    "        elif len(t) == 0: return len(s)\n",
    "        v0 = [None] * (len(t) + 1)\n",
    "        v1 = [None] * (len(t) + 1)\n",
    "        for i in range(len(v0)):\n",
    "            v0[i] = i\n",
    "        for i in range(len(s)):\n",
    "            v1[0] = i + 1\n",
    "            for j in range(len(t)):\n",
    "                cost = 0 if s[i] == t[j] else 1\n",
    "                v1[j + 1] = min(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost)\n",
    "            for j in range(len(v0)):\n",
    "                v0[j] = v1[j]\n",
    "                \n",
    "        return v1[len(t)]\n",
    "    \n",
    "def levenshtein_threshhold(distance_threshhold, confidence):\n",
    "    return distance_threshhold * (1- pow(confidence, 2))\n",
    "\n",
    "def conf(count_2_queries, count_query):\n",
    "    return count_2_queries / count_query\n",
    "\n",
    "def conf_level(conf):\n",
    "    factor = math.floor(conf * 10)\n",
    "    l = len(confidences)\n",
    "    conf_level = 0\n",
    "    i = 0\n",
    "    while(i < l and conf >= confidences[i]):\n",
    "        conf_level += math.floor(factor / math.floor(confidences[i] * 10))\n",
    "        i += 1\n",
    "    return conf_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add confidence column to result data frame\n",
      "Add confidence_level column to result data frame\n",
      "Add similarity column to result data frame\n",
      "Filter queries with high similarity, using levenshtein algorithm\n",
      "Sort results\n",
      "repartition results_df by conf_level column\n",
      "Count number of results\n"
     ]
    }
   ],
   "source": [
    "print(\"Add confidence column to result data frame\")\n",
    "func_conf_udf = udf(conf, FloatType())\n",
    "results_df = results_df.withColumn('conf',func_conf_udf(results_df['count_2_queries'], results_df['count_query']))\n",
    "\n",
    "print(\"Add confidence_level column to result data frame\")\n",
    "func_conf_level_udf = udf(conf_level, IntegerType())\n",
    "results_df = results_df.withColumn('conf_level',func_conf_level_udf(results_df['conf']))\n",
    "\n",
    "print(\"Add similarity column to result data frame\")\n",
    "func_levenshtein_udf = udf(levenshtein, IntegerType())\n",
    "results_df = results_df.withColumn('similarity',func_levenshtein_udf(results_df['query'], results_df['query2']))\n",
    "\n",
    "print(\"Filter queries with high similarity, using levenshtein algorithm\")\n",
    "results_df = results_df.filter(results_df['similarity'] >= levenshtein_threshhold(levenshtein_distance_threshhold, lit(results_df['conf'])))\n",
    "\n",
    "print(\"Sort results\")\n",
    "results_df = results_df.orderBy(['conf_level', 'conf', 'similarity', 'count_2_queries', 'count_query'], ascending=False)\n",
    "\n",
    "print(\"repartition results_df by conf_level column\")\n",
    "results_df.repartition('conf_level')\n",
    "\n",
    "print(\"Count number of results\")\n",
    "num_of_results = results_df.count()\n",
    "print('num of results = ' + repr(num_of_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save results data frame as related_searches\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'related_searches'\n",
    "print('Save results data frame as ' + folder_name)\n",
    "results_df.coalesce(1).write\\\n",
    "            .partitionBy('conf_level')\\\n",
    "            .format(\"com.databricks.spark.csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .save(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=======================\n",
      "rules with confidence 1\n",
      "=======================\n",
      "\n",
      "\n",
      "1) recent ==> videosearch.launch.start, conf=1.000, #q1=41, #(q1 and q2)=41, similarity=20\n",
      "\n",
      "\n",
      "2) undeground-love.com ==> dorki.ya-hoo.biz, conf=1.000, #q1=8, #(q1 and q2)=8, similarity=15\n",
      "\n",
      "\n",
      "3) www.at ==> www.at&t.com, conf=1.000, #q1=9, #(q1 and q2)=9, similarity=6\n",
      "\n",
      "\n",
      "4) diconary ==> dictionary, conf=1.000, #q1=7, #(q1 and q2)=7, similarity=2\n",
      "\n",
      "\n",
      "5) vzwpix.com ==> viewpix.com, conf=1.000, #q1=7, #(q1 and q2)=7, similarity=2\n",
      "\n",
      "\n",
      "6) www.friendspayday.com ==> www.friendspaydy.com, conf=1.000, #q1=7, #(q1 and q2)=7, similarity=1\n",
      "\n",
      "\n",
      "7) localhookup.com ==> localhookupz.com, conf=1.000, #q1=6, #(q1 and q2)=6, similarity=1\n",
      "\n",
      "\n",
      "total: 7 rules\n",
      "--------------\n",
      "\n",
      "\n",
      "=========================================\n",
      "rules with confidence between 0.8 and 0.9\n",
      "=========================================\n",
      "\n",
      "\n",
      "1) aol screen names ==> screen names, conf=0.875, #q1=8, #(q1 and q2)=7, similarity=4\n",
      "\n",
      "\n",
      "2) letssingit ==> mycl.cravelyrics.com, conf=0.857, #q1=7, #(q1 and q2)=6, similarity=17\n",
      "\n",
      "\n",
      "3) evo.qksrv.net ==> susan miller, conf=0.833, #q1=6, #(q1 and q2)=5, similarity=12\n",
      "\n",
      "\n",
      "4) get out of debt planner ==> getoutofdebtplanner, conf=0.833, #q1=6, #(q1 and q2)=5, similarity=4\n",
      "\n",
      "\n",
      "total: 4 rules\n",
      "--------------\n",
      "\n",
      "\n",
      "=========================================\n",
      "rules with confidence between 0.6 and 0.8\n",
      "=========================================\n",
      "\n",
      "\n",
      "1) undeground-love.com ==> portal.lolhost.com, conf=0.778, #q1=9, #(q1 and q2)=7, similarity=14\n",
      "\n",
      "\n",
      "2) elliot yamin ==> american idol, conf=0.750, #q1=8, #(q1 and q2)=6, similarity=12\n",
      "\n",
      "\n",
      "3) badcock furniture ==> badcock, conf=0.750, #q1=8, #(q1 and q2)=6, similarity=10\n",
      "\n",
      "\n",
      "4) porn ==> porn videos, conf=0.750, #q1=8, #(q1 and q2)=6, similarity=7\n",
      "\n",
      "\n",
      "5) www.cis.ohio-state.edu ==> mime, conf=0.733, #q1=15, #(q1 and q2)=11, similarity=20\n",
      "\n",
      "\n",
      "6) sinus infection ==> walmart, conf=0.714, #q1=7, #(q1 and q2)=5, similarity=14\n",
      "\n",
      "\n",
      "7) craigslist boston ==> craigslist, conf=0.714, #q1=7, #(q1 and q2)=5, similarity=7\n",
      "\n",
      "\n",
      "8) bad day lyrics ==> mycl.cravelyrics.com, conf=0.688, #q1=16, #(q1 and q2)=11, similarity=13\n",
      "\n",
      "\n",
      "9) new york new york las vegas ==> las vegas, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=18\n",
      "\n",
      "\n",
      "10) www.audradella.com ==> plus size lingerie, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=18\n",
      "\n",
      "\n",
      "11) the picture people ==> walmart, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=17\n",
      "\n",
      "\n",
      "12) carl.sfo.int.travelocity.com ==> travelocity, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=17\n",
      "\n",
      "\n",
      "13) daniel powter lyrics ==> mycl.cravelyrics.com, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=16\n",
      "\n",
      "\n",
      "14) ads.admonitor.net ==> passover recipes, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=15\n",
      "\n",
      "\n",
      "15) ciara ==> destiny's child, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=14\n",
      "\n",
      "\n",
      "16) phoenix suns ==> arizona cardinals, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=14\n",
      "\n",
      "\n",
      "17) cartoonnetwork ==> kidswb, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=13\n",
      "\n",
      "\n",
      "18) harley davidson motorcycles ==> honda motorcycles, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=12\n",
      "\n",
      "\n",
      "19) undeground-love.com ==> girliezone.com, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=12\n",
      "\n",
      "\n",
      "20) american west airlines ==> united airlines, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "\n",
      "21) pete wentz ==> fall out boy, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "\n",
      "22) kitchenaid appliances ==> kitchenaid, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "\n",
      "23) limewire.com ==> rap music, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "\n",
      "24) adultfan.nexcess.net ==> adult fanfiction, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "\n",
      "25) us airway ==> travelocity, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=10\n",
      "\n",
      "\n",
      "26) saks ==> neimanmarcus, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=10\n",
      "\n",
      "\n",
      "27) bloomingdales ==> neimanmarcus, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=10\n",
      "\n",
      "\n",
      "28) tattoo pictures ==> tattoo, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=9\n",
      "\n",
      "\n",
      "29) american west airlines ==> southwest airlines, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=9\n",
      "\n",
      "\n",
      "30) maps ==> funny shit, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=9\n",
      "\n",
      "\n",
      "31) internet ==> internet settings, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=9\n",
      "\n",
      "\n",
      "32) sears ==> sears hardware, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=9\n",
      "\n",
      "\n",
      "33) tattoos ==> tattoo pictures, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=8\n",
      "\n",
      "\n",
      "34) sci ==> dictionary, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=8\n",
      "\n",
      "\n",
      "35) matisyahu ==> matisyahu lyrics, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=7\n",
      "\n",
      "\n",
      "36) walmart ==> filene's, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=7\n",
      "\n",
      "\n",
      "37) www.dell.com ==> www.dellrebates.com, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=7\n",
      "\n",
      "\n",
      "38) slingo ==> 5 card slingo, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=7\n",
      "\n",
      "\n",
      "39) uranus ==> pluto, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\n",
      "\n",
      "\n",
      "40) jupiter ==> uranus, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\n",
      "\n",
      "\n",
      "41) 50 cent ==> ja rule, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\n",
      "\n",
      "\n",
      "42) saks fifth ave ==> neiman marcus, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=14\n",
      "\n",
      "\n",
      "43) www.ghana.gov.gh ==> ghana, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=11\n",
      "\n",
      "\n",
      "44) bombay kids ==> american idol, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=10\n",
      "\n",
      "\n",
      "45) song.com ==> jetblue.com, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=7\n",
      "\n",
      "\n",
      "46) walmart ==> fisher price, conf=0.615, #q1=13, #(q1 and q2)=8, similarity=11\n",
      "\n",
      "\n",
      "47) www.ninewest.com ==> nine west shoes, conf=0.615, #q1=13, #(q1 and q2)=8, similarity=10\n",
      "\n",
      "\n",
      "48) priceline.com ==> frontierairlines.com, conf=0.600, #q1=10, #(q1 and q2)=6, similarity=10\n",
      "\n",
      "\n",
      "total: 48 rules\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# Display a given character for a given length times\n",
    "def displayMultipleCharacters(character, title, length):\n",
    "    print(character * length)\n",
    "\n",
    "# Display title, with '=' decoration\n",
    "def displayTitle(title):\n",
    "    length = len(title)\n",
    "    print(\"\\n\")\n",
    "    displayMultipleCharacters('=', title, length)\n",
    "    print(title)\n",
    "    displayMultipleCharacters('=', title, length)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Display title, with dash decoration\n",
    "def displaySubTitle(title):\n",
    "    length = len(title)\n",
    "    print(title)\n",
    "    displayMultipleCharacters('-', title, length)\n",
    "\n",
    "# Display related results in a readable format: q1 ==> q2, with relevant statistics & math info.\n",
    "def display_results_list(results_df):\n",
    "    result_list = [list(row) for row in results_df.collect()]\n",
    "    previous_conf_level = 4\n",
    "    count_current_conf_results = 0\n",
    "\n",
    "    first_confidence = result_list[0][5] \n",
    "    if(first_confidence < 1):\n",
    "        title = 'rules with confidence between ' + repr(first_confidence) + ' and 1'\n",
    "    else:\n",
    "        title = 'rules with confidence 1'\n",
    "        \n",
    "    displayTitle(title)\n",
    "    \n",
    "    for i in range(num_of_results):\n",
    "        item = result_list[i]\n",
    "        current_conf_level = item[5]\n",
    "        if i > 0:\n",
    "            previous_conf_level = result_list[i-1][5]\n",
    "\n",
    "        count_current_conf_results += 1\n",
    "\n",
    "        if previous_conf_level != current_conf_level:\n",
    "            if i > 0:\n",
    "                displaySubTitle('total: ' + repr(count_current_conf_results - 1) + ' rules')\n",
    "\n",
    "            count_current_conf_results = 1\n",
    "            displayTitle(\"rules with confidence between \" +  repr(confidences[current_conf_level - 1]) + \" and \" + repr(confidences[current_conf_level]))\n",
    "\n",
    "        print('{index:d}) {q1} ==> {q2}, conf={confidence:.3f}, #q1={q1_count}, #(q1 and q2)={combined_count}, similarity={similarity}'.format(index = count_current_conf_results, q1 = item[0], q2 = item[1], confidence = item[4],  q1_count = item[3], combined_count = item[2], similarity = item[6]))\n",
    "        \n",
    "        if i < num_of_results - 1:\n",
    "            print('\\n')\n",
    "\n",
    "    if i == num_of_results - 1:\n",
    "        displaySubTitle('\\n\\ntotal: ' + repr(count_current_conf_results) + ' rules')\n",
    "\n",
    "if num_of_results < display_rules_num_of_records_threshhold:\n",
    "    display_results_list(results_df)\n",
    "else:\n",
    "    results_df.show(n2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[query: string, query2: string, count_2_queries: bigint, count_query: bigint, conf: float, conf_level: int, similarity: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Free memory\")\n",
    "results_df.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
