{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import count, col, length, asc, udf, lit\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s, t):\n",
    "        ''' From Wikipedia article; Iterative with two matrix rows. '''\n",
    "        if s == t: return 0\n",
    "        elif len(s) == 0: return len(t)\n",
    "        elif len(t) == 0: return len(s)\n",
    "        v0 = [None] * (len(t) + 1)\n",
    "        v1 = [None] * (len(t) + 1)\n",
    "        for i in range(len(v0)):\n",
    "            v0[i] = i\n",
    "        for i in range(len(s)):\n",
    "            v1[0] = i + 1\n",
    "            for j in range(len(t)):\n",
    "                cost = 0 if s[i] == t[j] else 1\n",
    "                v1[j + 1] = min(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost)\n",
    "            for j in range(len(v0)):\n",
    "                v0[j] = v1[j]\n",
    "                \n",
    "        return v1[len(t)]\n",
    "    \n",
    "def levenshtein_threshhold(distance_threshhold, confidence):\n",
    "    return distance_threshhold * (1- math.pow(confidence, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data\n"
     ]
    }
   ],
   "source": [
    "print(\"Load the data\")\n",
    "users_queries_search_main_df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .csv(\"user-ct-test-collection-01.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define global variables\n"
     ]
    }
   ],
   "source": [
    "print(\"Define global variables\")\n",
    "n1 = 2 # min num of rows to display\n",
    "n2 = 20 # max num of rows to display\n",
    "min_num_of_queries = 6\n",
    "min_num_of_queries_pair = 3\n",
    "min_num_of_chars_in_query = 2\n",
    "empty_queries = ['-', 'null']\n",
    "confidences = [0.6, 0.8, 0.9, 1]\n",
    "levenshtein_distance_threshhold = 15\n",
    "display_rules_num_of_records_threshhold = 200\n",
    "stop_queries = ['.com', 'google', 'google.com', 'www.google.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in the dataset, after initial filtering & dropping duplicates is: 1642464\n",
      "+----+----------------------------+\n",
      "|user|query                       |\n",
      "+----+----------------------------+\n",
      "|1337|michael keaton date of birth|\n",
      "|2334|disneychanne.com            |\n",
      "+----+----------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "repartition users_queries_df by user column\n",
      "cache users_queries_df\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user: string, query: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_queries_df = users_queries_search_main_df.select('AnonID', 'Query')\\\n",
    "                    .drop_duplicates(subset=['AnonID', 'Query'])\\\n",
    "                    .filter((col('Query').isin(empty_queries) == False) & (col('Query').isin(stop_queries) == False) & (length(col(\"Query\")) >= min_num_of_chars_in_query))\\\n",
    "                    .select(col('AnonID').alias('user'), col('Query').alias('query'))\n",
    "\n",
    "num_of_rows = users_queries_df.count()\n",
    "print(\"number of rows in the dataset, after initial filtering & dropping duplicates is: \" + repr(num_of_rows))\n",
    "users_queries_df.show(n1, truncate=False)\n",
    "\n",
    "print(\"repartition users_queries_df by user column\")\n",
    "users_queries_df.repartition('user')\n",
    "\n",
    "print(\"cache users_queries_df\")\n",
    "users_queries_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of queries, after filtering is: 12591\n",
      "+------------------+-----------+\n",
      "|query             |count_query|\n",
      "+------------------+-----------+\n",
      "|www.capitalone.com|112        |\n",
      "|black pussy       |21         |\n",
      "+------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "repartition queries_count_df by query column\n",
      "cache queries_count_df\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[query: string, count_query: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_count_df = users_queries_df.groupBy('query').agg(count(\"*\").alias(\"count_query\"))\\\n",
    "                    .filter(\"count_query > \" + repr(min_num_of_queries))\n",
    "\n",
    "num_of_queries = queries_count_df.count()\n",
    "print(\"number of queries, after filtering is: \" + repr(num_of_queries))\n",
    "queries_count_df.show(n1, truncate=False)\n",
    "\n",
    "print(\"repartition queries_count_df by query column\")\n",
    "queries_count_df.repartition('query')\n",
    "\n",
    "print(\"cache queries_count_df\")\n",
    "queries_count_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pairwise queries is: 16608\n",
      "+------------------+-----------------------+---------------+\n",
      "|query             |query2                 |count_2_queries|\n",
      "+------------------+-----------------------+---------------+\n",
      "|maps              |kelly blue book        |11             |\n",
      "|myspace.com       |area codes             |11             |\n",
      "|usps              |ticketmaster           |12             |\n",
      "|verizon           |kelly blue book        |9              |\n",
      "|yahoo messenger   |limewire               |9              |\n",
      "|yellow pages      |adobe                  |4              |\n",
      "|msn.com           |bank of america        |8              |\n",
      "|yahoo mail        |goggle.com             |4              |\n",
      "|florida lottery   |dillards               |5              |\n",
      "|nba               |ask jeeves             |5              |\n",
      "|ups tracking      |mapquest               |10             |\n",
      "|lowes             |burlington coat factory|10             |\n",
      "|reverse directory |mapquest               |11             |\n",
      "|clipart           |american idol          |4              |\n",
      "|royal caribbean   |best buy               |6              |\n",
      "|yahoo             |samsclub.com           |4              |\n",
      "|mapquest          |home shopping network  |7              |\n",
      "|myrtle beach      |mapquest               |15             |\n",
      "|myspace.com       |love quotes            |7              |\n",
      "|southwest airlines|monster.com            |6              |\n",
      "+------------------+-----------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "repartition queries_pair_count_df by query column\n",
      "Free memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user2: string, query2: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_queries_df2 = users_queries_df.select(col('user').alias('user2'), col('query').alias('query2'))\n",
    "users_queries_df2.repartition('user2')\n",
    "users_queries_df2.cache()\n",
    "filter_condition = \"count_2_queries > \" + repr(min_num_of_queries_pair)\\\n",
    "                     + \" and levenshtein(query, query2) >= \" + repr(levenshtein_threshhold(levenshtein_distance_threshhold,confidences[0]))\n",
    "queries_pair_count_df = users_queries_df2.join(users_queries_df,on=[col('user') == col('user2'), col('query') > col('query2')], how=('cross'))\\\n",
    "                                .select(col('user'), col('query'), col('query2'))\\\n",
    "                                .groupBy('query', 'query2').agg(count(\"*\").alias(\"count_2_queries\"))\\\n",
    "                                .filter(filter_condition)\n",
    "\n",
    "# Do not cache queries_pair_count_df, since it contains a cartesian join, which is too large to be cached.\n",
    "\n",
    "num_of_pairwise_queries = queries_pair_count_df.count()\n",
    "print(\"number of pairwise queries is: \" + repr(num_of_pairwise_queries))\n",
    "queries_pair_count_df.show(n2, truncate=False)\n",
    "\n",
    "print(\"repartition queries_pair_count_df by query column\")\n",
    "queries_pair_count_df.repartition('query')\n",
    "\n",
    "print(\"Free memory\")\n",
    "users_queries_df.unpersist()\n",
    "users_queries_df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add similarity column to result data frame\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[query: string, query2: string, count_2_queries: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Add similarity column to result data frame\")\n",
    "func_levenshtein_udf = udf(levenshtein, IntegerType())\n",
    "result_df = queries_pair_count_df.join(queries_count_df, on='query', how='inner')\\\n",
    "            .withColumn('similarity',func_levenshtein_udf(queries_pair_count_df['query'], queries_pair_count_df['query2']))\n",
    "\n",
    "queries_pair_count_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[query: string, query2: string, count_2_queries: string, count_query: string, similarity: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result_df.withColumn(\"count_2_queries\", result_df[\"count_2_queries\"].cast(StringType()))\n",
    "result_df = result_df.withColumn(\"count_query\", result_df[\"count_query\"].cast(StringType()))\n",
    "result_df = result_df.withColumn(\"similarity\", result_df[\"similarity\"].cast(StringType()))\n",
    "result_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of results is: 16573\n",
      "+-----------------+--------------------+---------------+-----------+----------+\n",
      "|query            |query2              |count_2_queries|count_query|similarity|\n",
      "+-----------------+--------------------+---------------+-----------+----------+\n",
      "|aruba            |american idol       |4              |28         |10        |\n",
      "|ask jeeves.com   |american idol.com   |6              |171        |12        |\n",
      "|ask jeeves.com   |aaa                 |4              |171        |13        |\n",
      "|badcock furniture|badcock             |6              |11         |10        |\n",
      "|goody's          |american idol       |4              |10         |13        |\n",
      "|medicare         |bank of america     |5              |82         |12        |\n",
      "|medicare         |continental airlines|7              |82         |16        |\n",
      "|medicare         |american airlines   |4              |82         |10        |\n",
      "|movie times      |http                |5              |62         |10        |\n",
      "|movie times      |ebay                |5              |62         |10        |\n",
      "|nextel phones    |map quest           |4              |12         |11        |\n",
      "|nextel phones    |mapquest            |5              |12         |11        |\n",
      "|pools            |american idol       |5              |27         |12        |\n",
      "|sams club        |people search       |4              |166        |12        |\n",
      "|sams club        |dictionary          |11             |166        |10        |\n",
      "|sams club        |cheap flights       |4              |166        |11        |\n",
      "|sams club        |disney channel.com  |4              |166        |14        |\n",
      "|sams club        |autotrader          |4              |166        |10        |\n",
      "|sams club        |american airlines   |5              |166        |14        |\n",
      "|sams club        |bank of america     |15             |166        |13        |\n",
      "+-----------------+--------------------+---------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_results = result_df.count()\n",
    "print(\"number of results is: \" + repr(num_of_results))\n",
    "result_df.show(n2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================\n",
      "12 rules with confidence between 0.6 and 0.8\n",
      "============================================\n",
      "\n",
      "1) saks fifth ave ==> neiman marcus, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=14\n",
      "\n",
      "2) www.continental airlines ==> mapquest, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=22\n",
      "\n",
      "3) www.cis.ohio-state.edu ==> mime, conf=0.733, #q1=15, #(q1 and q2)=11, similarity=20\n",
      "\n",
      "4) bombay kids ==> american idol, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=10\n",
      "\n",
      "5) www.ghana.gov.gh ==> ghana, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=11\n",
      "\n",
      "6) mortgage loans ==> ebay, conf=0.714, #q1=7, #(q1 and q2)=5, similarity=12\n",
      "\n",
      "7) tmobil ==> myspace.com, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=11\n",
      "\n",
      "8) destiny's child ==> ciara, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=14\n",
      "\n",
      "9) elliot yamin ==> american idol, conf=0.750, #q1=8, #(q1 and q2)=6, similarity=12\n",
      "\n",
      "10) server.tmrack.com ==> myspace.com, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=11\n",
      "\n",
      "11) northern tool ==> mapquest, conf=0.667, #q1=15, #(q1 and q2)=10, similarity=11\n",
      "\n",
      "12) myspace music videos ==> myspace layouts, conf=0.750, #q1=8, #(q1 and q2)=6, similarity=11\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "1 rule with confidence between 0.8 and 0.9\n",
      "============================================\n",
      "\n",
      "1) www.sprint.com mynextel ==> mapquest, conf=0.857, #q1=7, #(q1 and q2)=6, similarity=20\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "1 rule with confidence between 0.9 and 1\n",
      "============================================\n",
      "\n",
      "1) videosearch.launch.start ==> recent, conf=1.000, #q1=41, #(q1 and q2)=41, similarity=20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(confidences) - 1):\n",
    "    conf = confidences[i]\n",
    "    next_conf = confidences[i + 1]\n",
    "    filter_results_condition = 'count_2_queries / count_query > ' + repr(conf)\\\n",
    "                                + ' and count_2_queries / count_query <= ' + repr(next_conf)\n",
    "    \n",
    "    if i > 0:\n",
    "        filter_results_condition += ' and levenshtein(query, query2) >= ' + repr(levenshtein_threshhold(levenshtein_distance_threshhold,conf))\n",
    "    \n",
    "    current_result_df = result_df.filter(filter_results_condition)\n",
    "    current_result_df.orderBy(['count_2_queries', 'count_query'], ascending=False)\n",
    "    file_name = \"results_conf\" + repr(conf) + \"_to_\" + repr(next_conf) + \".txt\"\n",
    "    current_result_df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode(\"overwrite\").save(file_name)\n",
    "    \n",
    "    confidence_count = current_result_df.count()\n",
    "    if confidence_count < display_rules_num_of_records_threshhold:\n",
    "        str_rule = 'rule'\n",
    "        if(confidence_count != 1):\n",
    "            str_rule += 's'\n",
    "        print(\"\\n\\n============================================\")\n",
    "        print(repr(confidence_count) + \" \" + str_rule + \" with confidence between \" + repr(conf) + \" and \" + repr(next_conf))\n",
    "        print(\"============================================\\n\")\n",
    "        current_result_list = current_result_df.rdd.collect()\n",
    "        j = 1\n",
    "        for item in current_result_list:\n",
    "           print('{index:d}) {q1} ==> {q2}, conf={confidence:.3f}, #q1={q1_count}, #(q1 and q2)={combined_count}, similarity={similarity}\\n'.format(index = j, q1 = item[0], q2 = item[1], confidence = int(item[2])/int(item[3]),  q1_count = item[3], combined_count = item[2], similarity = item[4]))\n",
    "           j += 1\n",
    "    else:\n",
    "        print(\"\\n\\n============================================================\")\n",
    "        print(\"number of results with confidence between \" + repr(conf) + \" and \" + repr(next_conf) + \" is: \" + repr(confidence_count))\n",
    "        print(\"============================================================\\n\")\n",
    "        current_result_dict.show(n2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[query: string, query2: string, count_2_queries: string, count_query: string, similarity: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Free memory\")\n",
    "result_df.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
