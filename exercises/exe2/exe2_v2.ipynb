{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n==========\\nExercise 2\\n==========\\n\\nAssociation rules learning algorithm for related queries:\\n---------------------------------------------------------\\n\\ninput: list of pairs of (user id, search query)\\noutput: For query Sx: {Sx} ==> {Sy} with confidence Cxy\\n\\nDefinitions:\\n------------\\n\\nn - number of users\\nL[i] - number of queries of user i\\n1<=i<=n\\n1<=j<=L[i]\\nr - total number of queries\\nr = sigma(L[i]) [i=1..n]\\n1<=k,k1,k2<=r\\n\\nu[i] - user i\\nq[i,j] - query j of user i\\ncount(q) - count number of occurences of q in the data set\\ncount(q[k1] V q[k2]) - count number of occurences of query q[k1] and query q[k2] in the data set\\n\\nAlgorithm:\\n----------\\n\\nSupp(x) = |{tϵT; x ⊆ t}| / (|T|)\\n\\nConf(x V y) = Supp(y V x) / Supp(x) = |{tϵT; x,y ⊆ t}| / |{tϵT; x ⊆ t}|\\n\\n\\nAlgorithm implementation high level:\\n------------------------------------\\n1. Build users_queries_df, which consist of (u[i], q[i,j])\\n2. Build user_queries_count_df, which consist of (q[i,j], u[i], count(q[i,j]))\\n3. Build queries_count_df, that consist of (q[k], count(q[k]))\\n4. Build users_pair_queries_count_df, which consist of ((q[k1], q[k2]), count(q[k1] V q[k2]))\\n5. Build results_df, which consist of (q[k1], q[k2], count(q[k1] V q[k2]), count(q[k1])])\\n6. Add statistics & math columns to results_df\\n7. Display results\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "==========\n",
    "Exercise 2\n",
    "==========\n",
    "\n",
    "Association rules learning algorithm for related queries:\n",
    "---------------------------------------------------------\n",
    "\n",
    "input: list of pairs of (user id, search query)\n",
    "output: For query Sx: {Sx} ==> {Sy} with confidence Cxy\n",
    "\n",
    "Definitions:\n",
    "------------\n",
    "\n",
    "n - number of users\n",
    "L[i] - number of queries of user i\n",
    "1<=i<=n\n",
    "1<=j<=L[i]\n",
    "r - total number of queries\n",
    "r = sigma(L[i]) [i=1..n]\n",
    "1<=k,k1,k2<=r\n",
    "\n",
    "u[i] - user i\n",
    "q[i,j] - query j of user i\n",
    "count(q) - count number of occurences of q in the data set\n",
    "count(q[k1] V q[k2]) - count number of occurences of query q[k1] and query q[k2] in the data set\n",
    "\n",
    "Algorithm:\n",
    "----------\n",
    "\n",
    "Supp(x) = |{tϵT; x ⊆ t}| / (|T|)\n",
    "\n",
    "Conf(x V y) = Supp(y V x) / Supp(x) = |{tϵT; x,y ⊆ t}| / |{tϵT; x ⊆ t}|\n",
    "\n",
    "\n",
    "Algorithm implementation high level:\n",
    "------------------------------------\n",
    "1. Build users_queries_df, which consist of (u[i], q[i,j])\n",
    "2. Build user_queries_count_df, which consist of (q[i,j], u[i], count(q[i,j]))\n",
    "3. Build queries_count_df, that consist of (q[k], count(q[k]))\n",
    "4. Build users_pair_queries_count_df, which consist of ((q[k1], q[k2]), count(q[k1] V q[k2]))\n",
    "5. Build results_df, which consist of (q[k1], q[k2], count(q[k1] V q[k2]), count(q[k1])])\n",
    "6. Add statistics & math columns to results_df\n",
    "7. Display results\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext # required for reading input dataset & stop queries.\n",
    "from pyspark.sql.functions import count, col, length, udf, pow, lit, broadcast\n",
    "# count is required for aggregating queries, by number of occurences in the data set.\n",
    "# col is required for filtering out stop websites,  selecteing columns in data frames filters.\n",
    "# length is required for filtering out queries with small length.\n",
    "# udf is required for adding columns to results data frame\n",
    "# pow is required for calculating levenshtein threshold of confidence column\n",
    "# lit is required for calling the function that calculates levenshtein distance threshold of confidence column\n",
    "# broadcast is required for the removal of stop queries, which includes, using join between 2 data frames.\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "# IntegerType is required for adding confidence_level & similarity columns to results data frame, using udf\n",
    "# FloatType is required for adding confidence column to results data frame, using udf\n",
    "from pyspark.sql import Row # required for converting pair of queries, and their count, from rdd to data frame\n",
    "import math # required for calculating the floor for: 1) minimum number of queries pair. 2) confidence level\n",
    "from itertools import combinations # Required for creating combinations of queries for each user\n",
    "from operator import add # Required in reduce operation, for counting number of occurences of queries pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define global variables\n"
     ]
    }
   ],
   "source": [
    "print(\"Define global variables\")\n",
    "# min num of rows to display\n",
    "n1 = 10\n",
    "\n",
    "# max num of rows to display\n",
    "n2 = 20\n",
    "\n",
    "# Sorted list of confidences\n",
    "confidences = [0.6, 0.8, 0.9, 1]\n",
    "\n",
    "# Minimum number of characters in each query.\n",
    "min_num_of_chars_in_query = 2\n",
    "\n",
    "# Minimum number of occurences of a aingle query.\n",
    "min_num_of_queries = 6\n",
    "\n",
    "# Minimum number of occurences of pair of queries for different users.\n",
    "min_num_of_queries_pair =  math.floor(min_num_of_queries * confidences[0])\n",
    "\n",
    "# threshold for similarity between 2 queries\n",
    "levenshtein_distance_threshold = 10\n",
    "\n",
    "# threshold for collecting results as list (in order to display the results on the console in a readable format.)\n",
    "display_rules_num_of_records_threshold = 400\n",
    "\n",
    "# Irrlevant websites, that probably not related directly to search queries.\n",
    "stop_websites = 'google|gmail|mapquest|ebay|myspace|yahoo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAlgorithm implementation, in details:\\n-------------------------------------\\n\\n0. Read input data\\n    0.1 Read input users_queries_search_main_df with headers\\n    0.2 Read stop_queries_df without headers\\n\\n1. Build users_queries_df, which consist of (u[i], q[i,j])\\n    1.1 select only user & query columns, from users_queries_search_main_df, and set it into users_queries_df\\n    1.1 filter out queries that contains stop websites\\n    1.2 filter out queries with length less than min_num_of_chars_in_query\\n    1.3 remove duplicate entries\\n    1.4 filter out stop queries (related to the whole query, rather than string inside the query)\\n        1.4.1 set stop_queries_broadcast to be the broadcast of stop_queries_df\\n        1.4.2 set unwanted_queries_df to be the join of users_queries_df and stop_queries_broadcast, by query column\\n        1.4.3 users_queries_df = users_queries_df - unwanted_queries_df\\n\\n2. Build user_queries_count_df, which consist of (q[i,j], u[i], count(q[i,j]))\\n    2.1 join users_queries_df with queries_count_df, by query column\\n    \\n3. Build queries_count_df, that consist of (q[k], count(q[k]))\\n    3.1 aggregate group user_queries_count_df by query, and count queries\\n    3.2 filter out queries with number of occurences less than min_num_of_queries\\n    3.3 remove duplicate entries of ('query', 'count_query')\\n\\n4. Build users_pair_queries_count_df, which consist of ((q[k1], q[k2]), count(q[k1] V q[k2]))\\n    by performing the following operations:\\n    4.1 map: map user to suitable queries. (query, user, count_query) ==> (user, query)\\n    4.2 reduceByKey: For each user add the suitable list of queries. ((u1, [q1,q2,...]), (u2, [q2,q5,...]), ...)\\n    4.3 map: create pairs of queries combinations, for each user & drop users. ((q1,q2),(q1,q5),(q2,q5)...)\\n    4.4 flatMap add 1 to each query pair ((q1,q2),(q1,q5),(q2,q5)...) ==> (((q1,q2),1),((q1,q5),1),((q2,q5),1)...)\\n    4.5 reduceByKey count num of occurences of each query pair (((q1,q2),3),((q1,q5),7),((q2,q5),102)...)\\n    4.6 filter filter out queries, with number of occurences below min_num_of_queries_pair.\\n        For example: if min_num_of_queries_pair = 6, then the tuple ((q1,q2),3) will be filtered out.\\n\\n5. Build results_df, which consist of (q[k1], q[k2], count(q[k1] V q[k2]), count(q[k1])])\\n    5.1 Build query_results_df, that consist of (q[k1], q[k2], count(q[k1] V q[k2]), count(q[k1])])\\n        5.1.1 join users_pair_queries_count_df with queries_count_df, by query column\\n        5.1.2 filter queries with confidence less than the minimum confidence (which is confidences[0])\\n    5.2 Build query2_results_df, that consist of (q[k2], q[k1], count(q[k1] V q[k2]), count(q[k2])])\\n        5,2,1 join users_pair_queries_count_df with queries_count_df, by query column (rename query to query2)\\n        5.2.2 filter queries with confidence less than the minimum confidence (which is confidences[0])\\n    5.3 results_df is the union of query_results_df and query2_results_df, based on the key column (the first column)\\n\\n6. Add statistics & math columns to results_df\\n    6.1 Add confidence column:\\n        6.1.1 confidence(q[k1], q[k2]) = count(q[k1] V q[k2]) / count(q[k1])\\n    6.2 Add confidence_level column:\\n        6.2.1 confidence_level = sigma(floor(conf * 10) / floor(confidences[i] * 10)), i=0.. len(confidences)\\n        In case of confidences = [0.6, 0.8, 0.9, 1], we can simplify the definition to:\\n        confidence_level(confidence) = {\\n            1, if confidence = 1\\n            2, if confidence in range [0.9,1)\\n            3, if confidence in range [0.8,0.9)\\n            4, if confidence in range [0.6,0.8)\\n        }\\n    6.3 Add similarity column:\\n        6.3.1 similarity = levenshtein(q[k1], q[k2])\\n    6.4 Filter out queries with high similrity (high similrity <==> low scale of the levenshtein score) and low confidence\\n        6.4.1 filter out queries if:\\n                levenshtein(q[k1], q[k2]) < distance_threshhold * (1- pow(confidence(q[k1], q[k2]), 2))\\n\\n7. Display results\\n    7.1 Save results_df to disk partioned by confidence level\\n    7.2 Display results on the screen\\n        7.2.1 If number of results is less than display_rules_num_of_records_threshhold, then:\\n                Collect results as list and display it in the following format:\\n                q[k1] ==> q[k2], conf=[confidence(q[k1], q[k2])], #q1=[count(q[k1])], #(q1 and q2)=[count(q[k1] V q[k2])], similarity=[similarity(q[k1], q[k2])]\\n        7.2.2 Else: print results_df\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Algorithm implementation, in details:\n",
    "-------------------------------------\n",
    "\n",
    "0. Read input data\n",
    "    0.1 Read input users_queries_search_main_df with headers\n",
    "    0.2 Read stop_queries_df without headers\n",
    "\n",
    "1. Build users_queries_df, which consist of (u[i], q[i,j])\n",
    "    1.1 select only user & query columns, from users_queries_search_main_df, and set it into users_queries_df\n",
    "    1.1 filter out queries that contains stop websites\n",
    "    1.2 filter out queries with length less than min_num_of_chars_in_query\n",
    "    1.3 remove duplicate entries\n",
    "    1.4 filter out stop queries (related to the whole query, rather than string inside the query)\n",
    "        1.4.1 set stop_queries_broadcast to be the broadcast of stop_queries_df\n",
    "        1.4.2 set unwanted_queries_df to be the join of users_queries_df and stop_queries_broadcast, by query column\n",
    "        1.4.3 users_queries_df = users_queries_df - unwanted_queries_df\n",
    "\n",
    "2. Build user_queries_count_df, which consist of (q[i,j], u[i], count(q[i,j]))\n",
    "    2.1 join users_queries_df with queries_count_df, by query column\n",
    "    \n",
    "3. Build queries_count_df, that consist of (q[k], count(q[k]))\n",
    "    3.1 aggregate group user_queries_count_df by query, and count queries\n",
    "    3.2 filter out queries with number of occurences less than min_num_of_queries\n",
    "    3.3 remove duplicate entries of ('query', 'count_query')\n",
    "\n",
    "4. Build users_pair_queries_count_df, which consist of ((q[k1], q[k2]), count(q[k1] V q[k2]))\n",
    "    by performing the following operations:\n",
    "    4.1 map: map user to suitable queries. (query, user, count_query) ==> (user, query)\n",
    "    4.2 reduceByKey: For each user add the suitable list of queries. ((u1, [q1,q2,...]), (u2, [q2,q5,...]), ...)\n",
    "    4.3 map: create pairs of queries combinations, for each user & drop users. ((q1,q2),(q1,q5),(q2,q5)...)\n",
    "    4.4 flatMap add 1 to each query pair ((q1,q2),(q1,q5),(q2,q5)...) ==> (((q1,q2),1),((q1,q5),1),((q2,q5),1)...)\n",
    "    4.5 reduceByKey count num of occurences of each query pair (((q1,q2),3),((q1,q5),7),((q2,q5),102)...)\n",
    "    4.6 filter filter out queries, with number of occurences below min_num_of_queries_pair.\n",
    "        For example: if min_num_of_queries_pair = 6, then the tuple ((q1,q2),3) will be filtered out.\n",
    "\n",
    "5. Build results_df, which consist of (q[k1], q[k2], count(q[k1] V q[k2]), count(q[k1])])\n",
    "    5.1 Build query_results_df, that consist of (q[k1], q[k2], count(q[k1] V q[k2]), count(q[k1])])\n",
    "        5.1.1 join users_pair_queries_count_df with queries_count_df, by query column\n",
    "        5.1.2 filter queries with confidence less than the minimum confidence (which is confidences[0])\n",
    "    5.2 Build query2_results_df, that consist of (q[k2], q[k1], count(q[k1] V q[k2]), count(q[k2])])\n",
    "        5,2,1 join users_pair_queries_count_df with queries_count_df, by query column (rename query to query2)\n",
    "        5.2.2 filter queries with confidence less than the minimum confidence (which is confidences[0])\n",
    "    5.3 results_df is the union of query_results_df and query2_results_df, based on the key column (the first column)\n",
    "\n",
    "6. Add statistics & math columns to results_df\n",
    "    6.1 Add confidence column:\n",
    "        6.1.1 confidence(q[k1], q[k2]) = count(q[k1] V q[k2]) / count(q[k1])\n",
    "    6.2 Add confidence_level column:\n",
    "        6.2.1 confidence_level = sigma(floor(conf * 10) / floor(confidences[i] * 10)), i=0.. len(confidences)\n",
    "        In case of confidences = [0.6, 0.8, 0.9, 1], we can simplify the definition to:\n",
    "        confidence_level(confidence) = {\n",
    "            1, if confidence = 1\n",
    "            2, if confidence in range [0.9,1)\n",
    "            3, if confidence in range [0.8,0.9)\n",
    "            4, if confidence in range [0.6,0.8)\n",
    "        }\n",
    "    6.3 Add similarity column:\n",
    "        6.3.1 similarity = levenshtein(q[k1], q[k2])\n",
    "    6.4 Filter out queries with high similrity (high similrity <==> low scale of the levenshtein score) and low confidence\n",
    "        6.4.1 filter out queries if:\n",
    "                levenshtein(q[k1], q[k2]) < distance_threshhold * (1- pow(confidence(q[k1], q[k2]), 2))\n",
    "\n",
    "7. Display results\n",
    "    7.1 Save results_df to disk partioned by confidence level\n",
    "    7.2 Display results on the screen\n",
    "        7.2.1 If number of results is less than display_rules_num_of_records_threshhold, then:\n",
    "                Collect results as list and display it in the following format:\n",
    "                q[k1] ==> q[k2], conf=[confidence(q[k1], q[k2])], #q1=[count(q[k1])], #(q1 and q2)=[count(q[k1] V q[k2])], similarity=[similarity(q[k1], q[k2])]\n",
    "        7.2.2 Else: print results_df\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data set\n",
      "Load stop queries\n"
     ]
    }
   ],
   "source": [
    "print(\"Load the data set\")\n",
    "users_queries_search_main_df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .csv(\"user-ct-test-collection-01.txt\")\n",
    "\n",
    "print(\"Load stop queries\")\n",
    "stop_queries_df = spark.read.option(\"header\", \"false\")\\\n",
    "    .option(\"delimiter\", \"\\n\")\\\n",
    "    .csv(\"stop-queries.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter out queries that contains words from stop_websites or queries with length below 2 characters.\n",
      "Get data frame of (user, query), with the above filter & frop duplicates of (user, query)\n",
      "create a broadcast from stop_queries_df\n",
      "Join users_queries_df with stop_queries_broadcast, by the query column\n",
      "repartition users_queries_df by user column\n",
      "Get query count data frame, and filter out queries with number of occurences below 6\n",
      "repartition queries_count_df by query column\n",
      "Join users_queries_df with users_queries_df, by query column, to get a data frame of (query, user, count_query)\n"
     ]
    }
   ],
   "source": [
    "print(\"Filter out queries that contains words from stop_websites or \\\n",
    "queries with length below \" + repr(min_num_of_chars_in_query) + \" characters.\")\n",
    "users_queries_filter_condition = (col('Query').rlike(stop_websites) == False) & \\\n",
    "                                (length(col(\"Query\")) >= min_num_of_chars_in_query)\n",
    "\n",
    "print(\"Get data frame of (user, query), with the above filter & frop duplicates of (user, query)\")\n",
    "users_queries_df = users_queries_search_main_df.select('AnonID', 'Query')\\\n",
    "                    .drop_duplicates(subset=['AnonID', 'Query'])\\\n",
    "                    .filter(users_queries_filter_condition)\\\n",
    "                    .select(col('AnonID').alias('user'), col('Query').alias('query'))\n",
    "\n",
    "print(\"create a broadcast from stop_queries_df\")\n",
    "stop_queries_broadcast = broadcast(stop_queries_df.select(col('_c0').alias('query')))\n",
    "                                   \n",
    "print(\"Join users_queries_df with stop_queries_broadcast, by the query column\")\n",
    "unwanted_queries_df = users_queries_df.join(stop_queries_broadcast, on='query' , how = 'inner').select('user', 'query')\n",
    "users_queries_df = users_queries_df.subtract(unwanted_queries_df)\n",
    "\n",
    "print(\"repartition users_queries_df by user column\")\n",
    "users_queries_df.repartition('user')\n",
    "\n",
    "print(\"Get query count data frame, and filter out queries with number of occurences below \" + repr(min_num_of_queries))\n",
    "queries_count_df = users_queries_df.groupBy('query').agg(count(\"*\").alias(\"count_query\"))\\\n",
    "                    .filter(\"count_query >= \" + repr(min_num_of_queries))\\\n",
    "                    .drop_duplicates(subset=['query', 'count_query'])               \n",
    "\n",
    "print(\"repartition queries_count_df by query column\")\n",
    "queries_count_df.repartition('query')\n",
    "\n",
    "print(\"Join users_queries_df with users_queries_df, by query column, to get a data frame of (query, user, count_query)\")\n",
    "users_queries_count_df = users_queries_df.join(queries_count_df, on='query', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert users_pair_queries_count_rdd to data frame, in order to join it with queries_count_df\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "users_pair_queries_count_df is converted to rdd,\n",
    "in order to avoid self cartesian join (which is required in order to get all of the combinations of pair of queries)\n",
    "by performing the following operations:\n",
    ".map: map user to suitable queries. (query, user, count_query) ==> (user, query)\n",
    ".reduceByKey: For each user add the suitable list of queries. ((u1, [q1,q2,...]), (u2, [q2,q5,...]), ...)\n",
    ".map: create pairs of queries combinations, for each user & drop users. ((q1,q2),(q1,q5),(q2,q5)...)\n",
    ".flatMap add 1 to each query pair ((q1,q2),(q1,q5),(q2,q5)...) ==> (((q1,q2),1),((q1,q5),1),((q2,q5),1)...)\n",
    ".reduceByKey count num of occurences of each query pair (((q1,q2),3),((q1,q5),7),((q2,q5),102)...)\n",
    ".filter filter out queries, with number of occurences below min_num_of_queries_pair.\n",
    "        For example: if min_num_of_queries_pair = 6, then the tuple ((q1,q2),3) will be filtered out.\n",
    "'''\n",
    "users_pair_queries_count_rdd = users_queries_count_df.rdd\\\n",
    "                        .map(lambda line: (line[1], [line[0]]))\\\n",
    "                        .reduceByKey(add)\\\n",
    "                        .map(lambda line: tuple(combinations(line[1], 2)))\\\n",
    "                        .flatMap(lambda line: [(x, 1) for x in line])\\\n",
    "                        .reduceByKey(add)\\\n",
    "                        .filter(lambda line: line[1] >= min_num_of_queries_pair)\n",
    "\n",
    "print(\"Convert users_pair_queries_count_rdd to data frame, in order to join it with queries_count_df\")\n",
    "users_pair_queries_count_df = sqlContext.createDataFrame(users_pair_queries_count_rdd.map(lambda line: Row(query=line[0][0], query2=line[0][1], count_2_queries=line[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join query count to results data frame\n"
     ]
    }
   ],
   "source": [
    "print(\"Join query count to results data frame\")\n",
    "query_results_df = users_pair_queries_count_df.join(queries_count_df, on = 'query', how = 'inner')\\\n",
    "                                        .filter('count_2_queries / count_query >= ' + repr(confidences[0]))\\\n",
    "                                        .select('query', 'query2', 'count_2_queries', 'count_query')\n",
    "\n",
    "query2_results_df = users_pair_queries_count_df.join(queries_count_df.select(col('query').alias('query2'), col('count_query')), on = 'query2', how = 'inner')\\\n",
    "                                        .filter('count_2_queries / count_query >= ' + repr(confidences[0]))\\\n",
    "                                        .select('query', 'query2', 'count_2_queries', 'count_query')\n",
    "\n",
    "results_df = query_results_df.union(query2_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity between 2 strings.\n",
    "#\n",
    "# Parameters:\n",
    "# s - first string\n",
    "# t - second string\n",
    "#\n",
    "# The similarity between s and t is,\n",
    "# the minimum number of single-character edits (i.e. insertions, deletions, or substitutions),\n",
    "# required to change one word into the other.\n",
    "# Therefore low number of edit actions means high similarirty between s and t, and vice versa.\n",
    "def levenshtein(s, t):\n",
    "        ''' From Wikipedia article; Iterative with two matrix rows. '''\n",
    "        if s == t: return 0\n",
    "        elif len(s) == 0: return len(t)\n",
    "        elif len(t) == 0: return len(s)\n",
    "        v0 = [None] * (len(t) + 1)\n",
    "        v1 = [None] * (len(t) + 1)\n",
    "        for i in range(len(v0)):\n",
    "            v0[i] = i\n",
    "        for i in range(len(s)):\n",
    "            v1[0] = i + 1\n",
    "            for j in range(len(t)):\n",
    "                cost = 0 if s[i] == t[j] else 1\n",
    "                v1[j + 1] = min(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost)\n",
    "            for j in range(len(v0)):\n",
    "                v0[j] = v1[j]\n",
    "                \n",
    "        return v1[len(t)]\n",
    "\n",
    "# Get normalized levenshtein threshold, based on initial threshold and confidence.\n",
    "#\n",
    "# Parameters :\n",
    "# distance_threshold - initial levenshtein distance threshold\n",
    "# confidence - the confidence that 2 queries: q1,q2 are associated: q1 ==> q2\n",
    "#\n",
    "# The  levenshtein threshold is required for filtering queries, based on low similarity, and high confidence.\n",
    "# We would like to get results with confidence as high as possible, and similarity as low as possible.\n",
    "# Therefore for high confidence we can compromise the demand for low similarity, and vice versa.\n",
    "# This threshold should be compared with the similarity of 2 queries.\n",
    "# The requirement is to have a similarity no less than this threshold\n",
    "# Low similarity (high levenshtein value) will be accepted only with high confidence (and vice versa).\n",
    "def levenshtein_threshold(distance_threshold, confidence):\n",
    "    return distance_threshold * (1- pow(confidence, 2))\n",
    "\n",
    "# Calculate confidence of 2 queries.\n",
    "#\n",
    "# Parameters:\n",
    "# count_2_queries - number of occurences of 2 queries: q1, q2\n",
    "# count_query - number of occurences of a single query: q1\n",
    "def conf(count_2_queries, count_query):\n",
    "    return count_2_queries / count_query\n",
    "\n",
    "# partition the possible confidences to groups.\n",
    "# The ammount of groups, is the size of confidences list\n",
    "# conf_level = sigma(floor(conf * 10) / floor(confidences[i] * 10)), i=0.. len(confidences)\n",
    "def conf_level(conf):\n",
    "    factor = math.floor(conf * 10)\n",
    "    l = len(confidences)\n",
    "    conf_level = 0\n",
    "    i = 0\n",
    "    while(i < l and conf >= confidences[i]):\n",
    "        conf_level += math.floor(factor / math.floor(confidences[i] * 10))\n",
    "        i += 1\n",
    "    return conf_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add confidence column to result data frame\n",
      "Add confidence_level column to result data frame\n",
      "Add similarity column to result data frame\n",
      "Filter queries with high similarity, using levenshtein algorithm\n",
      "Sort results\n",
      "repartition results_df by conf_level column\n",
      "Count number of results\n",
      "num of results = 58\n"
     ]
    }
   ],
   "source": [
    "print(\"Add confidence column to result data frame\")\n",
    "func_conf_udf = udf(conf, FloatType())\n",
    "results_df = results_df.withColumn('conf',func_conf_udf(results_df['count_2_queries'], results_df['count_query']))\n",
    "\n",
    "print(\"Add confidence_level column to result data frame\")\n",
    "func_conf_level_udf = udf(conf_level, IntegerType())\n",
    "results_df = results_df.withColumn('conf_level',func_conf_level_udf(results_df['conf']))\n",
    "\n",
    "print(\"Add similarity column to result data frame\")\n",
    "func_levenshtein_udf = udf(levenshtein, IntegerType())\n",
    "results_df = results_df.withColumn('similarity',func_levenshtein_udf(results_df['query'], results_df['query2']))\n",
    "\n",
    "print(\"Filter queries with high similarity, using levenshtein algorithm\")\n",
    "results_df = results_df.filter(results_df['similarity'] >= levenshtein_threshold(levenshtein_distance_threshold, lit(results_df['conf'])))\n",
    "\n",
    "print(\"Sort results\")\n",
    "results_df = results_df.orderBy(['conf_level', 'conf', 'similarity', 'count_2_queries', 'count_query'], ascending=False)\n",
    "\n",
    "print(\"repartition results_df by conf_level column\")\n",
    "results_df.repartition('conf_level')\n",
    "\n",
    "print(\"Count number of results\")\n",
    "num_of_results = results_df.count()\n",
    "print('num of results = ' + repr(num_of_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save results data frame as related_searches\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'related_searches'\n",
    "print('Save results data frame as ' + folder_name)\n",
    "results_df.coalesce(1).write\\\n",
    "            .partitionBy('conf_level')\\\n",
    "            .format(\"com.databricks.spark.csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .save(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a given character for a given length times\n",
    "def get_display_multiple_characters(character, title, length):\n",
    "    return character * length\n",
    "\n",
    "# Display title, with '=' decoration\n",
    "def get_display_title(title):\n",
    "    length = len(title)\n",
    "    str_output = '\\n'\n",
    "    str_output += get_display_multiple_characters('=', title, length)\n",
    "    str_output += '\\n'\n",
    "    str_output += title\n",
    "    str_output += '\\n'\n",
    "    str_output += get_display_multiple_characters('=', title, length)\n",
    "    str_output += '\\n\\n'\n",
    "    return str_output\n",
    "    \n",
    "# Display title, with dash decoration\n",
    "def get_display_subTitle(title):\n",
    "    length = len(title)\n",
    "    str_output = '\\n'\n",
    "    str_output += title\n",
    "    str_output += '\\n'\n",
    "    str_output += get_display_multiple_characters('-', title, length)\n",
    "    str_output += '\\n\\n'\n",
    "    return str_output\n",
    "\n",
    "# Get display subtitle of current results, including number of results & confidence range\n",
    "def get_display_current_results_subTitle(count_current_conf_results, current_conf_level):\n",
    "    str_rules = 'rule'\n",
    "    if(count_current_conf_results != 1):\n",
    "        str_rules += 's'\n",
    "    return get_display_subTitle(repr(count_current_conf_results) + \" \" + str_rules + \" with confidence between \" +  repr(confidences[current_conf_level]) + \" and \" + repr(confidences[current_conf_level + 1]))\n",
    "    \n",
    "# Display related results in a readable format: q1 ==> q2, with relevant statistics & math info.\n",
    "def get_results_list(results_list):\n",
    "    previous_conf_level = 4\n",
    "    count_current_conf_results = 0\n",
    "    str_results = get_display_title('Related Queries')\n",
    "    str_items = ''\n",
    "    \n",
    "    for i in range(num_of_results):\n",
    "        item = results_list[i]\n",
    "        current_conf_level = item[5]\n",
    "        if i > 0:\n",
    "            previous_conf_level = results_list[i-1][5]\n",
    "\n",
    "        if previous_conf_level != current_conf_level:\n",
    "            str_results += get_display_current_results_subTitle(count_current_conf_results, current_conf_level)\n",
    "            str_results += str_items\n",
    "            str_items = ''\n",
    "            count_current_conf_results = 0\n",
    "\n",
    "        str_items += '{index:d}) {q1} ==> {q2}, conf={confidence:.3f}, #q1={q1_count}, #(q1 and q2)={combined_count}, similarity={similarity}\\n\\n'.format(index = count_current_conf_results + 1, q1 = item[0], q2 = item[1], confidence = item[4],  q1_count = item[3], combined_count = item[2], similarity = item[6])\n",
    "        count_current_conf_results += 1\n",
    "\n",
    "    str_results += get_display_current_results_subTitle(count_current_conf_results, current_conf_level - 1)\n",
    "    str_results += str_items\n",
    "    return str_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============\n",
      "Related Queries\n",
      "===============\n",
      "\n",
      "\n",
      "6 rules with confidence between 0.9 and 1\n",
      "-----------------------------------------\n",
      "\n",
      "1) undeground-love.com ==> dorki.ya-hoo.biz, conf=1.000, #q1=8, #(q1 and q2)=8, similarity=15\n",
      "\n",
      "2) www.at ==> www.at&t.com, conf=1.000, #q1=9, #(q1 and q2)=9, similarity=6\n",
      "\n",
      "3) diconary ==> dictionary, conf=1.000, #q1=7, #(q1 and q2)=7, similarity=2\n",
      "\n",
      "4) vzwpix.com ==> viewpix.com, conf=1.000, #q1=7, #(q1 and q2)=7, similarity=2\n",
      "\n",
      "5) www.friendspayday.com ==> www.friendspaydy.com, conf=1.000, #q1=7, #(q1 and q2)=7, similarity=1\n",
      "\n",
      "6) localhookup.com ==> localhookupz.com, conf=1.000, #q1=6, #(q1 and q2)=6, similarity=1\n",
      "\n",
      "\n",
      "4 rules with confidence between 0.8 and 0.9\n",
      "-------------------------------------------\n",
      "\n",
      "1) aol screen names ==> screen names, conf=0.875, #q1=8, #(q1 and q2)=7, similarity=4\n",
      "\n",
      "2) letssingit ==> mycl.cravelyrics.com, conf=0.857, #q1=7, #(q1 and q2)=6, similarity=17\n",
      "\n",
      "3) evo.qksrv.net ==> susan miller, conf=0.833, #q1=6, #(q1 and q2)=5, similarity=12\n",
      "\n",
      "4) get out of debt planner ==> getoutofdebtplanner, conf=0.833, #q1=6, #(q1 and q2)=5, similarity=4\n",
      "\n",
      "\n",
      "48 rules with confidence between 0.6 and 0.8\n",
      "--------------------------------------------\n",
      "\n",
      "1) undeground-love.com ==> portal.lolhost.com, conf=0.778, #q1=9, #(q1 and q2)=7, similarity=14\n",
      "\n",
      "2) elliot yamin ==> american idol, conf=0.750, #q1=8, #(q1 and q2)=6, similarity=12\n",
      "\n",
      "3) badcock furniture ==> badcock, conf=0.750, #q1=8, #(q1 and q2)=6, similarity=10\n",
      "\n",
      "4) porn ==> porn videos, conf=0.750, #q1=8, #(q1 and q2)=6, similarity=7\n",
      "\n",
      "5) www.cis.ohio-state.edu ==> mime, conf=0.733, #q1=15, #(q1 and q2)=11, similarity=20\n",
      "\n",
      "6) sinus infection ==> walmart, conf=0.714, #q1=7, #(q1 and q2)=5, similarity=14\n",
      "\n",
      "7) craigslist boston ==> craigslist, conf=0.714, #q1=7, #(q1 and q2)=5, similarity=7\n",
      "\n",
      "8) bad day lyrics ==> mycl.cravelyrics.com, conf=0.688, #q1=16, #(q1 and q2)=11, similarity=13\n",
      "\n",
      "9) new york new york las vegas ==> las vegas, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=18\n",
      "\n",
      "10) www.audradella.com ==> plus size lingerie, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=18\n",
      "\n",
      "11) the picture people ==> walmart, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=17\n",
      "\n",
      "12) carl.sfo.int.travelocity.com ==> travelocity, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=17\n",
      "\n",
      "13) daniel powter lyrics ==> mycl.cravelyrics.com, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=16\n",
      "\n",
      "14) ads.admonitor.net ==> passover recipes, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=15\n",
      "\n",
      "15) ciara ==> destiny's child, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=14\n",
      "\n",
      "16) phoenix suns ==> arizona cardinals, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=14\n",
      "\n",
      "17) cartoonnetwork ==> kidswb, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=13\n",
      "\n",
      "18) harley davidson motorcycles ==> honda motorcycles, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=12\n",
      "\n",
      "19) undeground-love.com ==> girliezone.com, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=12\n",
      "\n",
      "20) american west airlines ==> united airlines, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "21) pete wentz ==> fall out boy, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "22) kitchenaid appliances ==> kitchenaid, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "23) limewire.com ==> rap music, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "24) adultfan.nexcess.net ==> adult fanfiction, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=11\n",
      "\n",
      "25) us airway ==> travelocity, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=10\n",
      "\n",
      "26) saks ==> neimanmarcus, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=10\n",
      "\n",
      "27) bloomingdales ==> neimanmarcus, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=10\n",
      "\n",
      "28) tattoo pictures ==> tattoo, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=9\n",
      "\n",
      "29) american west airlines ==> southwest airlines, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=9\n",
      "\n",
      "30) maps ==> funny shit, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=9\n",
      "\n",
      "31) internet ==> internet settings, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=9\n",
      "\n",
      "32) sears ==> sears hardware, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=9\n",
      "\n",
      "33) tattoos ==> tattoo pictures, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=8\n",
      "\n",
      "34) sci ==> dictionary, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=8\n",
      "\n",
      "35) matisyahu ==> matisyahu lyrics, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=7\n",
      "\n",
      "36) walmart ==> filene's, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=7\n",
      "\n",
      "37) www.dell.com ==> www.dellrebates.com, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=7\n",
      "\n",
      "38) slingo ==> 5 card slingo, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=7\n",
      "\n",
      "39) uranus ==> pluto, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\n",
      "\n",
      "40) jupiter ==> uranus, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\n",
      "\n",
      "41) 50 cent ==> ja rule, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\n",
      "\n",
      "42) saks fifth ave ==> neiman marcus, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=14\n",
      "\n",
      "43) www.ghana.gov.gh ==> ghana, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=11\n",
      "\n",
      "44) bombay kids ==> american idol, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=10\n",
      "\n",
      "45) song.com ==> jetblue.com, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=7\n",
      "\n",
      "46) walmart ==> fisher price, conf=0.615, #q1=13, #(q1 and q2)=8, similarity=11\n",
      "\n",
      "47) www.ninewest.com ==> nine west shoes, conf=0.615, #q1=13, #(q1 and q2)=8, similarity=10\n",
      "\n",
      "48) priceline.com ==> frontierairlines.com, conf=0.600, #q1=10, #(q1 and q2)=6, similarity=10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if num_of_results < display_rules_num_of_records_threshold:\n",
    "    results_list = [list(row) for row in results_df.collect()]\n",
    "    print(get_results_list(results_list))\n",
    "else:\n",
    "    results_df.show(n2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[query: string, query2: string, count_2_queries: bigint, count_query: bigint, conf: float, conf_level: int, similarity: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Free memory\")\n",
    "results_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    ======================================\\n    Interesting relations in the data set:\\n    ======================================\\n    \\n    1 interesting relation in confidence 1\\n    ---------------------------------------\\n    undeground-love.com ==> dorki.ya-hoo.biz, conf=1.000, #q1=8, #(q1 and q2)=8, similarity=15\\n    \\n    2 interesting relations in confidence between 0.8 and 0.9\\n    ---------------------------------------------------------\\n    letssingit ==> mycl.cravelyrics.com, conf=0.857, #q1=7, #(q1 and q2)=6, similarity=17\\n    evo.qksrv.net ==> susan miller, conf=0.833, #q1=6, #(q1 and q2)=5, similarity=12\\n    \\n    6 interesting relations in confidence between 0.6 and 0.8\\n    ----------------------------------------------------------\\n    elliot yamin ==> american idol\\n    harley davidson motorcycles ==> honda motorcycles, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=12\\n    bombay kids ==> american idol, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=10\\n    50 cent ==> ja rule, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\\n    uranus ==> pluto, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\\n    jupiter ==> uranus, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\\n    \\n    There are no interestin relations in confidence between 0.9 and 1\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    ======================================\n",
    "    Interesting relations in the data set:\n",
    "    ======================================\n",
    "    \n",
    "    1 interesting relation in confidence 1\n",
    "    ---------------------------------------\n",
    "    undeground-love.com ==> dorki.ya-hoo.biz, conf=1.000, #q1=8, #(q1 and q2)=8, similarity=15\n",
    "    \n",
    "    2 interesting relations in confidence between 0.8 and 0.9\n",
    "    ---------------------------------------------------------\n",
    "    letssingit ==> mycl.cravelyrics.com, conf=0.857, #q1=7, #(q1 and q2)=6, similarity=17\n",
    "    evo.qksrv.net ==> susan miller, conf=0.833, #q1=6, #(q1 and q2)=5, similarity=12\n",
    "    \n",
    "    6 interesting relations in confidence between 0.6 and 0.8\n",
    "    ----------------------------------------------------------\n",
    "    elliot yamin ==> american idol\n",
    "    harley davidson motorcycles ==> honda motorcycles, conf=0.667, #q1=9, #(q1 and q2)=6, similarity=12\n",
    "    bombay kids ==> american idol, conf=0.625, #q1=8, #(q1 and q2)=5, similarity=10\n",
    "    50 cent ==> ja rule, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\n",
    "    uranus ==> pluto, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\n",
    "    jupiter ==> uranus, conf=0.667, #q1=6, #(q1 and q2)=4, similarity=6\n",
    "    \n",
    "    There are no interestin relations in confidence between 0.9 and 1\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
